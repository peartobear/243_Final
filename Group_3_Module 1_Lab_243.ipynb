{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"am0lL86bbjs8"},"source":["# <p style=\"text-align: center;\">News NLP and Summarization</p>\n","# <p style=\"text-align: center;\">Module-1</p>\n","\n","# <p style=\"text-align: center;\">Group - 3</p>\n","### <p style=\"text-align: center;\"> Members: Adedeji Aluko, Ankur Kumar, Apurva Arni, Bora Unalmis, Jack O'Donoghue, Kashin Shah </p>\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"DzpexTONbjs-"},"source":["## Agenda\n","1. Project Description and Objectives\n","2. Description of Data and Sources\n","3. Data Preprocessing\n","4. Data Analysis\n","5. Summary and Next Steps"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"pi5pUI2Gbjs_"},"source":["## Project Description and Objective\n","\n","In the world with overwhelming amount of information, people often find it harder to keep up with the news cycles. Therefore, a machine learning model which can adequately summarize information would enable people to quickly and efficiently digest vast amounts of information, keeping them informed and up-to-date on current events in a fast-paced world. The challenge of distilling complex news stories into concise, contextual, and accurate summaries presents a fascinating opportunity for artificial intelligence and NLP advancements.\n","\n","To achive the stated goal in the data analysis part, we will use NLP toolkit to preprocess the data. We will then use the K-Means Clustering algorithm to categorize groups of news articles based on similarities in their text content. Further, we will use supervised classification algorithms to anticipate which cluster a new article would belong to most closely. Additionally, we will conduct pairwise cosine similarity on a user-inputted article to generate extractive text summarization that provides the reader with a brief overview of the articleâ€™s key points.\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"WiX8MF0ebjs_"},"source":["## Description of Data and Sources\n","\n","In this project, we used the combination of readily available datasets and the scraped data. We decided to use the sitemap to scrap the news websites and merged it to build a large dataset which is 8.4 GB in size. It contains 2,688,878 news articles and essays from 27 different news publications in between the years 2016 and 2020. Since the dataset is large in size and becuase we had memory bottleneck, we divided it in thousands of chunks and used random chunk to perform exploratory data analysis."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"iG-nhSmkbjs_"},"source":["The data included in every dataframe are:\n","\n","* **date** date the article was published in yy-mm-dd formate\n","* **year** year the article was published\n","* **month** month the article was published (in float)\n","* **day** day the article was published\n","* **author** author of the article\n","* **title** title of the article\n","* **article** the body of the article"]},{"cell_type":"markdown","metadata":{"id":"elXNBfZMbjs_"},"source":["![Screenshot%202023-02-13%20at%209.39.05%20PM.png](attachment:Screenshot%202023-02-13%20at%209.39.05%20PM.png)"]},{"cell_type":"code","execution_count":1,"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"4fXt0oTUbjtA","executionInfo":{"status":"error","timestamp":1677393306071,"user_tz":480,"elapsed":543,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}},"outputId":"9b170f32-c74f-4f32-b946-41b645adaec5"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6550ece8bfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/ankur/Downloads/all-the-news-2-1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ankur/Downloads/all-the-news-2-1.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#%pip install wordcloud\n","from wordcloud import WordCloud, STOPWORDS\n","\n","for chunk in pd.read_csv('/Users/ankur/Downloads/all-the-news-2-1.csv', chunksize=500000):\n","    df = pd.DataFrame(chunk)\n","    if df.empty == False:\n","        break\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"pVNno3qjbjtB"},"source":["Before cleaning the data, we worked on the shape of the random dataframe chunk with size 500,000. This effectively means that we are reading 500,000 rows of dataset at a time in a single chunk. As evident from the shape, we have 500,000 rows and 10 columns.The exploratory data analysis that we performed is on this 50,000 chunk. In the below steps, we will be cleaning the data set before performing the exploratory analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"Q_I_lnvabjtB","executionInfo":{"status":"aborted","timestamp":1677393306072,"user_tz":480,"elapsed":16,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"4QSFCcvnbjtB"},"source":["### Data Pre-processing (Data Cleaning)"]},{"cell_type":"markdown","metadata":{"id":"5Wi3MFwJbjtB"},"source":["For the data cleaning, we followed the steps which are explained after each cleaning process in the sub section.\n","1. We started off by reorganizing the columns to promote clarity and comprehension of each row, and omitted any unnecessary columns.\n","2. Then a column was created to account for the length of each article."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"ztiewHrqbjtB","executionInfo":{"status":"aborted","timestamp":1677393306072,"user_tz":480,"elapsed":16,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df= df[['title', 'author', 'article', 'year', 'month', 'day', 'section', 'publication']]"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"VpFpuqEqbjtB","executionInfo":{"status":"aborted","timestamp":1677393306072,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df['len_article'] = df.article.str.len()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"MV1IS2m1bjtB","executionInfo":{"status":"aborted","timestamp":1677393306073,"user_tz":480,"elapsed":16,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"cNPnvvNqbjtB","executionInfo":{"status":"aborted","timestamp":1677393306073,"user_tz":480,"elapsed":16,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df['len_article'] = df.article.str.len()\n","df_new = df.dropna().copy()\n","df_new.reset_index(inplace=True, drop=True)"]},{"cell_type":"markdown","metadata":{"id":"rFl93g4SbjtC"},"source":["3. The dataset was checked for null values and any rows containing NAN were deleted."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":false,"id":"0Ht2ki6bbjtC","executionInfo":{"status":"aborted","timestamp":1677393306073,"user_tz":480,"elapsed":16,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df_new.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"r-Igq83MbjtC","executionInfo":{"status":"aborted","timestamp":1677393306073,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["print(\"The original dataset has shape of\", df.shape)\n","print(\"The new dataset has shape of\", df_new.shape)"]},{"cell_type":"markdown","metadata":{"id":"2lja_QCTbjtC"},"source":["We realized that the rows with NAN values made up 72% of this chunk of dataset. Due to the large proportion of NAN values in the original dataset, both datasets were used selectively to ensure a comprehensive exploratory data analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":true,"id":"Ljpx5RzPbjtC","executionInfo":{"status":"aborted","timestamp":1677393306073,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df.groupby('publication').article.count()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"EUiUl-5nbjtC"},"source":["The graph shows the frequency of occurrence of various word count values in a set of articles. The x-axis represents the number of articles, and the y-axis represents the word count of each article.\n","\n","The graph has a mean value of 3152 words, indicated by a horizontal line intersecting the y-axis at 3152. This line serves as a reference point, providing insight into the average word count in the articles. The graph also shows the spread of the word count values for each article, with the distribution ranging from a minimum word count to a maximum word count for each article."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"bEZ2EAZxbjtC"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"fHeyu_ykbjtC","executionInfo":{"status":"aborted","timestamp":1677393306074,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"ftHIfvajbjtC"},"source":["### Word Count\n","\n","Keeping in mind the major objective of this project, we consider using two datasets (one with minimal cleaning and a dataset with no NAN values) to understand the distribution of the word count and the effect the removal of all NAN values have on the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"o3svOk28bjtC","executionInfo":{"status":"aborted","timestamp":1677393306074,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["# Close to a normal distribution, with a positive skew\n","plt.figure(figsize=(16,8))\n","mean = df.len_article.mean()\n","plt.axvline(mean, ls='--', color='black')\n","sns.histplot(df.len_article)\n","plt.xlim([0, 12500])\n","#plt.ylim([0, 5000])\n","plt.xlabel('Word Count')\n","plt.ylabel('Numbers of Articles')\n","plt.title(f'Distribution of Word Count (Mean: {mean:.0f} words)', fontsize=18);"]},{"cell_type":"markdown","metadata":{"id":"aFDlUK_9bjtC"},"source":["The graph has a mean value of 3152 words, indicated by a horizontal line intersecting the y-axis at 3152. This line serves as a reference point, providing insight into the average word count in the articles. The graph also shows the spread of the word count values for each article, with the distribution ranging from a minimum word count to a maximum word count for each article."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"0WbpF9ZkbjtD","executionInfo":{"status":"aborted","timestamp":1677393306074,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["# Close to a normal distribution, with a positive skew\n","plt.figure(figsize=(16,8))\n","mean = df_new.len_article.mean()\n","plt.axvline(mean, ls='--', color='black')\n","sns.histplot(df_new.len_article)\n","plt.xlim([0, 12500])\n","#plt.ylim([0, 5000])\n","plt.xlabel('Word Count')\n","plt.ylabel('Numbers of Articles')\n","plt.title(f'Distribution of Word Count with Cleaner Dataset(Mean: {mean:.0f} words)', fontsize=18);"]},{"cell_type":"markdown","metadata":{"id":"8TP0zhffbjtD"},"source":["The graph shows the frequency of occurrence of various word count values in a set of articles. The x-axis represents the number of articles, and the y-axis represents the word count of each article.\n","\n","Both features appeared normally distributed in general with a heavy positive skew. What we found interesting was that the mean word count for the cleaner datasets (4218) was greater than that of the dataset with minimal cleaning (3152). We speculated that this resulted from the removed rows having more word count lesser than 2000 as can be seen in the first figure."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"8d1sbmhNbjtD"},"source":["### Word Cloud\n","\n","\n","Our data about articles is rich in content regarding a variety of topics from several different publications. To gain insight into what this content focuses on, we decided to create word clouds. Word clouds are visualizations that reveal essential details about the text being analyzed. The size of a word in the visualization determines the frequency of that specific word in the text. This gives the audience an overall sense of the text and is a good indicator of themes that have been talked about.\n","\n","Word cloud uses the cleaned dataframe built after dropping the null values."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"VuCCTL-FbjtD"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"GxjKHtL2bjtD","executionInfo":{"status":"aborted","timestamp":1677393306075,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["words = ['said', 'S', 'Vice', 'Reuters', 'The Verge', 'now', 'told', 'week', 'would', 'new', 'take','year', 'will', 'much', 'time', 'one']\n","\n","for _ in words:\n","    STOPWORDS.add(_)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"B8hNEGcAbjtD","executionInfo":{"status":"aborted","timestamp":1677393306075,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["publications = df_new.publication.unique()\n","\n","for publication in publications:\n","    print(publication)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"sgWYXlXqbjtD"},"source":["For the chunk of data that we analyzed, we decided to segregate articles based on their publication to get a sense of the themes and topics that most articles seemed to focus on for that specific publication. Three unique publications were found for this chunk of data. The first word cloud is generic and is created from articles from all the publications. This word cloud has mixed themes, which was expected. The themes to note here are politics, business, everyday issues, and emerging events"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"jP1Ji3RKbjtD","executionInfo":{"status":"aborted","timestamp":1677393306075,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["text = df_new['article'].values\n","\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(str(text))\n","plt.figure(figsize=(16,8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"b08evqcdbjtD"},"source":["The first publication was The Vice. It promotes itself in covering stories that are not well covered by other publications. Based on the word cloud, the themes to note here are astrology and mental health. This was an interesting find because this publication covers several other themes like arts, culture and general news, but astrology and spirituality stood out for this period of time. Other news seems to be rare based on this word cloud."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"4EojQndIbjtD","executionInfo":{"status":"aborted","timestamp":1677393306075,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df_vice = df_new[df_new['publication'] == 'Vice']\n","df_reuters = df_new[df_new['publication'] == 'Reuters']\n","df_verge = df_new[df_new['publication'] == 'The Verge']"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"4NqK4do2bjtD","executionInfo":{"status":"aborted","timestamp":1677393306075,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["text = df_vice['article'].values\n","\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(str(text))\n","plt.figure(figsize=(16,8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"l_Hxs2JGbjtE"},"source":["The second publication Reuters. Reuters is known to be one of the largest news agencies in the world, and was established in London. Reuters provides current business, financial, national and international news. Based on the word cloud, it is evident that the articles had themes of politics, business and finance. A lot of the articles also seem to focus more on national news in England."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"dg3etxMBbjtE","executionInfo":{"status":"aborted","timestamp":1677393306076,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["text = df_reuters['article'].values\n","\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(str(text))\n","plt.figure(figsize=(16,8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Fn-_4_LebjtE"},"source":["The third publication was The Verge. It is an American technology news website that that features product reviews, consumer news, feature stories, etc. The world cloud has themes in line with this. There is current news, consumer news and reviews, product information, and other popular topics in the time period."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"QWaFgg5lbjtE","executionInfo":{"status":"aborted","timestamp":1677393306076,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["text = df_verge['article'].values\n","\n","wordcloud = WordCloud(stopwords=STOPWORDS).generate(str(text))\n","plt.figure(figsize=(16,8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"ukNX1HRpbjtE","executionInfo":{"status":"aborted","timestamp":1677393306076,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df['year'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"TXfP_IHKbjtE"},"source":["## Publication Date Analysis\n","\n","#### a) Number of Articles distributed based on the Year\n","We distributed our data into multiple chunks as discussed above. The EDA plots respresented below are based on the 500,000 row chunks from the raw data file. The below plot provides the representation of the number of articles published in each year from 2016 to 2019 based on the chuck used for the analysis. The highest number of articles published is in the year 2017 and the number of publications are as high as >150000 articles. The least number of articles published as per the analysis from the selected chunk is in the year 2019 and the number of publications are close to 80000 articles. The number of articles published has reduced from 2017 to 2018 by ~13.3% and the articles published from 2018 to 2019 is reduced by 36% which is comparitavely high. \n","\n","As future steps, we would like to further analyse how the trend looks like for the rest of the chunks. This will be an analysis out of the project scope. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"Ay6IzC8QbjtE","executionInfo":{"status":"aborted","timestamp":1677393306076,"user_tz":480,"elapsed":14,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["plt.rcParams['figure.figsize'] = [16, 8]\n","sns.set(font_scale = 1.2, style = 'whitegrid')\n","sns_year = sns.countplot(x=df['year'], palette = sns.color_palette(\"crest\"))\n","sns_year.set(xlabel = \"Year\", ylabel = \"Articles\", title = \"Number of the articles per year\")"]},{"cell_type":"markdown","metadata":{"id":"SoaaPud5bjtE"},"source":["#### b)Articles published per month in a year\n","\n","As per our analysis, we noticed that the number of articles published in June is the highest with 58997 number of articles published. The months - January, February, March, April, May and July have publications in between 43000 and 48000. The number of articles published have reduced to ~30000 from the months August to December based on our analysis. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEHEDEZQbjtE","executionInfo":{"status":"aborted","timestamp":1677393306077,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["month =  df['month']\n","count = Counter(month)\n","count = pd.Series(count).sort_values(ascending=False)\n","\n","bar_charts('Distribution of articles released per month', 'Month of the Year', 'Number of articles',\n","           count, count.keys(), (15, 10), 1, 18)"]},{"cell_type":"markdown","metadata":{"id":"I7aAKyqAbjtE"},"source":["#### c)Articles published per day in a month\n","\n","For the dataset in the chunk used for EDA, we notice that the highest number of publications are made in the second week of every month (especially on the 12th day). As we observe from the below plot, we notice that the least number of articles published is on the 31st day. The reason for this is that all the months in a year(i.e. February, April, June, September, November) do not have 31 days. Similarly, since February do not have days from 29 to 31 in all years except for in 2016 from the chunk of 500,000 data we used in the analysis, we have less number of articles published in those days.Otherwise, we notice that almost all days have more than or close to 15000 articles per day."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRI_0ZN6bjtE","executionInfo":{"status":"aborted","timestamp":1677393306077,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["plt.rcParams['figure.figsize'] = [17, 9]\n","sns.set(font_scale = 1, style = 'whitegrid')\n","df_source = df.day.value_counts()\n","sns.barplot(x=df_source.index, y=df_source, palette = sns.color_palette(\"crest\"))\n","plt.ylabel('Number of Articles', fontsize=15)\n","plt.xlabel('Days of the Month', fontsize=15)\n","plt.xticks(rotation=0)\n","plt.title('Distribution of Articles Per Day', fontsize=15);"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":true,"id":"YR1DlTvbbjtF","executionInfo":{"status":"aborted","timestamp":1677393306077,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df_new['author'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"CjkHP4YlbjtF"},"source":["### Publications Source Analysis\n","#### a)Top 10 Authors based on the number of publications\n","plot, we want to identify the top 10 authors based on the number of articles published by them. The top author who published the highest number of articles is Axios. Axios published more than 4000 articles and close to 5000 articles in the 500,000 chunk from the raw dataset used for this analysis. Sarah Perez, the second highest publisher with respect to the number of articles published is about 1600 articles less than the top publishing author Axios. You can further notice the other 8 authors and their number of publications in the below plot. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"eVlTccFFbjtF","executionInfo":{"status":"aborted","timestamp":1677393306077,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["plt.rcParams['figure.figsize'] = [16,8]\n","sns.set(font_scale = 1.2, style = 'whitegrid')\n","df_author = df.author.value_counts().head(10)\n","sns.barplot(x=df_author, y=df_author.index,palette = sns.color_palette(\"rocket_r\"))\n","plt.ylabel('Authors', fontsize=12)\n","plt.xlabel('Number of Articles', fontsize=12)\n","#plt.legend(fontsize=14)\n","plt.title('Top 10 Authors', fontsize=14);\n","#sns_year.set( ylabel = \"Number of Articles\", title = \"Top 10 Authur\")"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"4M9qKz93bjtF","executionInfo":{"status":"aborted","timestamp":1677393306077,"user_tz":480,"elapsed":15,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["for column in df_new:\n","    Y = df_new['publication']\n","Y.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"eRZKkagIbjtF","executionInfo":{"status":"aborted","timestamp":1677393306079,"user_tz":480,"elapsed":2171,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["def bar_charts(title, x, y, values, keys, figsize, color_num, fontsize=12):\n","    fig, ax = plt.subplots(1, 1, figsize=figsize)\n","    if color_num == 1:\n","        colors = sns.color_palette(\"crest\")\n","    else:\n","        colors = sns.color_palette(\"rocket_r\")\n","    bars = plt.bar(keys, values, color=colors)\n","\n","    for bar in bars:\n","        label = list(count)[list(bars).index(bar)]\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2, height, label, ha='center', va='bottom',\n","                 fontsize=fontsize)\n","\n","    plt.title(title, fontsize=fontsize)\n","    plt.xlabel(x, fontsize=fontsize)\n","    plt.ylabel(y, fontsize=fontsize)\n","    plt.xticks(rotation=45)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YhfVKnBgbjtF"},"source":["\n","#### b)Articles sourced based on publications\n","\n","The below plot provides us information on the number of articles sourced based on the publications of the articles from the data available in the 500,000 raw datafile chunk. The highest number of articles are published buy Reuters which are about 119988. This margin is greater by ~21.6% (from 119988 to 93980) to the second highest number of articles published by Vice. The third highest number of articles published are 80073 articles and is published by Refinery 29 followed by TechCrunch, TMZ, Vox, Axios for which the number of articles published are in between 40000 and 45000 articles. We are also considering Vice News and Vice as two separate entities in our analysis as both news articles are seperately published. Another interesting observation is that out of the chunk used for EDA, we noticed that article \"The Verge\" has only 2 publications. \n","\n","As next steps for the future out-of-scope project analysis, we would like to analyse if \"The Verge\" has the lowest number of publications compared to the rest of all other publications. If yes, we would like to further deep dive to figure out the margin by which the number of publications are low and the possible reason for the lower number of articles published in these years. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"GwibH4LwbjtF","executionInfo":{"status":"aborted","timestamp":1677393306079,"user_tz":480,"elapsed":2168,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["from collections import Counter\n","count = Counter(df['publication'])\n","count = pd.Series(count).sort_values(ascending=False)\n","#print(count)\n","keys = list(count.keys())\n","#print(keys)\n","bar_charts('Articles per source', 'Publication', 'Number of articles', count, keys,\n","          (20, 13), 2, 18)"]},{"cell_type":"markdown","metadata":{"id":"XRS060xObjtF"},"source":["#### c)Top 10 sections in the published articles \n","\n","One of the interesting analyses we performed is the analysis based on the various sections published by all the articles we sourced for the project. We observe that the market size or the customers of these articles display their interest in \"Tech\" and find it the most fascinating subject while \"Health\" is the least concerning topic for the audience. Therefore, the number of articles written on Tech are relatively higher than on Politics or health. "]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":false,"id":"NXjoJYvibjtF","executionInfo":{"status":"aborted","timestamp":1677393306080,"user_tz":480,"elapsed":2165,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["section = df_new['section']\n","count = Counter(section)\n","count = pd.Series(count).sort_values(ascending=False).head(11)\n","\n","bar_charts('Top 10 Sections of Published Articles', 'Sections', 'Number of articles', count, count.keys(), (16, 8), 2, 16)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"pyM6rH4FbjtF"},"source":["### Sentiment Analysis\n","\n","The goal of the sentiment analysis is to gauge the tone of each article.\n","Our initial method is to use Python's Natural Language Toolkit, to remove stopwords, strip, standardize, and lemmatize every article into a list of 'words'. Then these lists are parsed and the number of 'negative' and 'positive' words are counted. These words are defined by txt files downloaded from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html which references the following two papers:<br>\n","\n","Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" <br>\n","    Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA. <br>\n","\n","\n","\n","Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing and Comparing Opinions on the Web.\" <br>\n","\n","Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.<br>\n","\n","The sentiment score can then be calculated and normalized from the ratio of positive to negative words. Thus a positive value conveys a positive article tone. However, a positive word does not necessarily imply a positive tone, and furthermore many words may contain differing levels of positivity. Thus as we continue work on this project, we intend to explore VADER (Valence Aware and Sentiment Reasoner) tools to find a more accurate scaled value for polarity from negative and positive valence. https://www.analyticsvidhya.com/blog/2021/12/different-methods-for-calcul\n","ating-sentiment-score-of-text/. <br>\n","\n","Bing Liu. \"Sentiment Analysis and Subjectivity.\" An chapter in Handbook of Natural Language Processing, Second Edition, (editors: N. Indurkhya and F. J. Damerau), 2010."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"scrolled":true,"id":"gT7AYhYCbjtF","executionInfo":{"status":"aborted","timestamp":1677393306080,"user_tz":480,"elapsed":2162,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"55g40ttWbjtF","executionInfo":{"status":"aborted","timestamp":1677393306080,"user_tz":480,"elapsed":2159,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["for chunk in pd.read_csv('/Users/ankur/Downloads/all-the-news-2-1.csv', chunksize=50000):\n","    df_sen = pd.DataFrame(chunk)\n","    if df_sen.empty == False:\n","        break\n","n = 1\n","for chunk in pd.read_csv('/Users/ankur/Downloads/all-the-news-2-1.csv', chunksize=50000):\n","    dfx = pd.DataFrame(chunk)\n","    n = n+1\n","    if n == 20:\n","        break\n","\n","df_sen.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"QwatfYnUbjtG","executionInfo":{"status":"aborted","timestamp":1677393306080,"user_tz":480,"elapsed":2156,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["frames = [df_sen, dfx]\n","df_sen = pd.concat(frames)\n","df.head()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"etcJInCobjtG"},"source":["### Sentiment Analysis by Publication\n","\n","By separating the publication sources, we can assess the sentiment of their articles. We do not want to simply consider the average/mean tone of their pieces, but we can use that to sort by publication. Then a stacked ridge plot can be used to highlight the distribution of their articles' sentiment scores. What is clear from the plot is that the sentiment scores are mostly close to zero, and follow a relatively normal distribution. With a larger chunk of the dataset however, we may find more insights into how polarising certain sources are. We also may learn more when using the VADER tools to define sentiment scores. <br>\n","\n","Our hope is that by understanding the polarity and sentiment, we can train our deep learning model to summarize articles while maintaining the key points and the overall sentiment. Furthermore, the absolute count of negative and positive words (emotive language) may allow our model to somewhat mirror the style of writing of that publication."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"jiixgysabjtG","executionInfo":{"status":"aborted","timestamp":1677393306081,"user_tz":480,"elapsed":2154,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df_sen.groupby('publication')['month'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"1xQHQQlqbjtG","executionInfo":{"status":"aborted","timestamp":1677393306081,"user_tz":480,"elapsed":2153,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","lemma = WordNetLemmatizer()\n","stop_words = stopwords.words('english')\n","def text_prep(x: str) -> list:\n","     corp = str(x).lower()\n","     corp = re.sub('[^a-zA-Z]+',' ', corp).strip()\n","     tokens = word_tokenize(corp)\n","     words = [t for t in tokens if t not in stop_words]\n","     lemmatize = [lemma.lemmatize(w) for w in words]\n","     return lemmatize"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"FXAQ4TAebjtG","executionInfo":{"status":"aborted","timestamp":1677393306081,"user_tz":480,"elapsed":2152,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["preprocess_tag = [text_prep(i) for i in df_sen['article']]\n","df_sen[\"preprocess_txt\"] = preprocess_tag\n","df_sen['total_len'] = df_sen['preprocess_txt'].map(lambda x: len(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"Er-0e1hQbjtG","executionInfo":{"status":"aborted","timestamp":1677393306082,"user_tz":480,"elapsed":2150,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["file = open('/Users/ankur/Downloads/negative-words.txt', 'r')\n","neg_words = file.read().split()\n","file = open('/Users/ankur/Downloads/positive-words.txt', 'r')\n","pos_words = file.read().split()\n","num_pos = df_sen['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))\n","df_sen['pos_count'] = num_pos\n","num_neg = df_sen['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))\n","df_sen['neg_count'] = num_neg\n","df_sen['sentiment'] = round((df_sen['pos_count'] - df_sen['neg_count']) / df_sen['total_len'], 2)\n","df_sen.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"xOC8shKLbjtG","executionInfo":{"status":"aborted","timestamp":1677393306082,"user_tz":480,"elapsed":2149,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df_sen['sentiment'] = round((df_sen['pos_count'] - df_sen['neg_count']) / df_sen['total_len'], 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"ZkoTlk1jbjtG","executionInfo":{"status":"aborted","timestamp":1677393306082,"user_tz":480,"elapsed":2147,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df1 = df_sen.groupby('publication')['sentiment'].mean()\n","df1 = pd.DataFrame(df1.sort_values())\n","pubs = df1.index.tolist()\n","df_sen['publication'] = pd.Categorical(df_sen['publication'], categories = pubs)\n","df_sen = df_sen.sort_values(by=['publication'])\n","df1.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"sAVpcajmbjtG","executionInfo":{"status":"aborted","timestamp":1677393306082,"user_tz":480,"elapsed":2143,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["df1['color'] = (df1['sentiment']/0.05)*100+50\n","df1['color'] = df1['color'].astype(int)\n","palette = sns.color_palette(\"RdYlGn\", 100)\n","palette = [palette[i] for i in df1['color']]\n","df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"R6YK63XFbjtG","executionInfo":{"status":"aborted","timestamp":1677393306082,"user_tz":480,"elapsed":2140,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n","\n","g = sns.FacetGrid(data=df_sen, row=\"publication\", hue=\"publication\", palette=palette, aspect=10, height=1.5)\n","\n","# Draw the densities\n","g.map_dataframe(sns.kdeplot, \"sentiment\",\n","                bw_adjust=0.8, clip_on=False, fill=True, alpha=0.9, linewidth=5, multiple='layer')\n","g.map_dataframe(sns.kdeplot, x=\"sentiment\", bw_adjust=0.8, color='grey')\n","g.map(plt.axhline, y=0, lw=1, clip_on=False, color='black')\n","g.map(plt.axvline, x=0, lw=0.25, clip_on=False, color='grey')\n","g.map(plt.axvline, x=-0.05, lw=0.25, clip_on=False, color='grey')\n","g.map(plt.axvline, x=-0.1, lw=0.25, clip_on=False, color='grey')\n","g.map(plt.axvline, x=0.05, lw=0.25, clip_on=False, color='grey')\n","g.map(plt.axvline, x=0.1, lw=0.25, clip_on=False, color='grey')\n","g.map(plt.axvline, x=0.15, lw=0.25, clip_on=False, color='grey')\n","\n","# Define and use a simple function to label the plot in axes coordinates\n","def label(x, color, label):\n","    ax = plt.gca()\n","    ax.text(0, 0.2, x.iloc[0], fontweight=\"bold\", color='black',\n","           ha=\"left\", va=\"center\", transform=ax.transAxes)\n","\n","g.map(label, \"publication\")\n","\n","# Set the subplots to overlap\n","g.fig.subplots_adjust(hspace=-0.55)\n","\n","# Remove axes details that don't play well with overlap\n","g.set_titles(\"\")\n","g.set(yticks=[], xlabel=\"Article Aggregate Sentiment Score\", ylabel=\"\")\n","g.despine(bottom=False, left=True)\n","plt.xlim(-0.15,0.15)\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"QpDsq2p9bjtG"},"source":["### Sentiment Analysis by Date\n","\n","Another idea we have is to asses the trend in polarity and sentiment over time. We cannot tell much from the diagram below, and the high average sentiment scores at the end of the graph are simply due to a small sample of articles available from those dates. However, we do anticipate notable trends over time as politics grew more divisive from 2016-2021, and as the COVID-19 Pandemic took over the newstream in 2020."]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"nP2GrJTDbjtG","executionInfo":{"status":"aborted","timestamp":1677393306083,"user_tz":480,"elapsed":2138,"user":{"displayName":"Apurva Arni","userId":"02050354212755579835"}}},"outputs":[],"source":["from datetime import datetime\n","from datetime import datetime\n","import plotly.graph_objs as go\n","import plotly.express as px\n","\n","df_sen['new_date'] = pd.to_datetime(df_sen['date']).dt.date\n","df2 = df_sen.groupby('new_date')['sentiment'].mean()\n","df2 = pd.DataFrame(df2)\n","df2['negative'] = df_sen.groupby('new_date')['neg_count'].mean()\n","df2['positive'] = df_sen.groupby('new_date')['pos_count'].mean()\n","px.line(df2, x = df2.index, y = df2.columns)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"qQe1eMHQbjtH"},"source":["### Summary and Next Steps"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"U2fYyL84bjtH"},"source":["In this module, we have we found that working with a big dataset(8.8 GB) like that of ours will require specialized toolings like sqlite - as loading such a huge dataset and running loops on it individually will casue memory bottleneck. Further, by different visualizations like distribution of articles by date and top sections of the articles by publisher, we got a global view and understanding of the dataset. \n","\n","Furthermore, by running sensitivity and polarity analysis we were able to get valuable insights like Reuters having largely negative sentiment and entertaiment publishers like Mashable with positive sentiment. These insights will help us architect our machine learning models to closley maintain the context and sentiment around the summary. \n","\n","As a next step, we will train several clustering algorithms to categorize groups of news articles and supervized classification algorithms to predict which cluster a new article would belongs to. By benchmarking these algorithms against each other, we would be able to come up with the best perfoming model. "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}